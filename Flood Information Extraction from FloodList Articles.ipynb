{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12781c4-8278-4d44-84d9-3c630c129e6b",
   "metadata": {},
   "source": [
    "# üß† NLP Project: Flood Information Extraction from FloodList Articles (UK)\n",
    "\n",
    "## üåä Objective:\n",
    "To automatically extract flood-related information such as rainfall, water level, storm names, and locations from FloodList news articles, and identify whether ‚Äúflash floods‚Äù were mentioned.\n",
    "The project also organizes flood-related articles into thematic groups based on the presence of flash-flood mentions and quantitative data, helping identify which articles are most relevant or data-rich for further analysis.\n",
    "\n",
    "## üß© Workflow\n",
    "\n",
    "1. Load and inspect the dataset\n",
    "    Imported the CSV file containing full news texts.\n",
    "    Checked for missing or misnamed columns and confirmed the presence of \"Full Text\".\n",
    "\n",
    "2. Text preprocessing\n",
    "    Removed punctuation and English stopwords using NLTK.\n",
    "    Ensured non-string entries were handled safely (empty strings for NaNs).\n",
    "    Tokenized each article into lists of words.\n",
    "\n",
    "3. Detect flash-flood mentions\n",
    "    Scanned each tokenized article for the pattern 'flash' followed by 'flood'.\n",
    "    Created a binary column Flash_Flood_Mentioned (1 = present, 0 = absent).\n",
    "\n",
    "4. Information extraction with spaCy and regex\n",
    "    Used regular expressions to capture:\n",
    "    Rainfall values (e.g., ‚Äú45 mm‚Äù, ‚Äú20 millimetres‚Äù)\n",
    "    Water-level values (e.g., ‚Äú3.5 m‚Äù, ‚Äú2.8 metres‚Äù)\n",
    "    Storm names (e.g., ‚ÄúStorm Dennis‚Äù)\n",
    "    Applied spaCy NER to extract geographic locations (GPE entities).\n",
    "    Stored extracted values in new columns: Rainfall_mm, WaterLevel_m, StormName, Location.\n",
    "\n",
    "5. Feature flags and grouping\n",
    "    Added flags has_rainfall, has_waterlevel, and has_any_numeric.\n",
    "    Combined these with Flash_Flood_Mentioned to form four groups:\n",
    "        FlashFlood + Numerics\n",
    "        FlashFlood (no numerics)\n",
    "        OtherFlood + Numerics\n",
    "        OtherFlood (no numerics)\n",
    "\n",
    "6. Priority grouping and insights\n",
    "    Classified each article into the four groups for interpretability.\n",
    "    Highlighted the ‚ÄúFlashFlood + Numerics‚Äù subset as high-priority articles\n",
    "    (most informative for hydrological research or event validation).\n",
    "    Extracted top-mentioned locations from this subset.\n",
    "\n",
    "7. Export results of classification and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd0a2e1b-288e-4f7a-882c-638791d744ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\taran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All modules loaded and ready!\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# STEP 1 ‚Äî INSTALL & IMPORT MODULES\n",
    "# ====================================================\n",
    "\n",
    "#  Install required packages (run once per new environment)\n",
    "# Uncomment these if you get \"ModuleNotFoundError\"\n",
    "# !pip install pandas numpy matplotlib nltk spacy tqdm\n",
    "\n",
    "#  Download additional resources (only first time)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#  Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "#  Load SpaCy language model (download if not installed)\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"‚úÖ All modules loaded and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89ecdab3-78df-4971-bdb7-5718e2a79abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File loaded successfully.\n",
      "Number of articles: 80\n",
      "Column names: ['Title', 'Date', 'Full Text', 'Link']\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# STEP 2 ‚Äî LOAD THE DATA\n",
    "# ====================================================\n",
    "\n",
    "# File path (keep your CSV in the same folder or give full path)\n",
    "uk_article_file_csv = 'uk_flood_articles_80.csv'\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(uk_article_file_csv)\n",
    "\n",
    "# Quick basic check\n",
    "print(\"‚úÖ File loaded successfully.\")\n",
    "print(\"Number of articles:\", len(df))\n",
    "print(\"Column names:\", list(df.columns))\n",
    "\n",
    "# # Optional quick peek\n",
    "# print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6c7220b-0b3b-4093-a7e3-152bf3d0e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text rows: 80\n",
      "‚úÖ Text cleaned and tokenized.\n",
      "Example tokens from first article:\n",
      "['Parts', 'United', 'Kingdom', 'continue', 'grapple', 'widespread', 'flooding', 'stemming', 'passage', 'Storm', 'Babet', 'Authorities', 'confirmed', 'grim', 'toll', 'least', 'four', 'fatalities', 'linked', 'storm', 'swept', 'nation', 'recent', 'days', 'Hundreds', 'people', 'evacuated', 'homes', 'parts', 'Scotland']\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# STEP 3 ‚Äî CLEAN TEXT, REMOVE STOPWORDS, TOKENIZE\n",
    "# ====================================================\n",
    "\n",
    "# Prepare stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Convert text column to list\n",
    "full_text_list = df['Full Text'].tolist()\n",
    "row_count = len(full_text_list)\n",
    "print(\"Number of text rows:\", row_count)\n",
    "\n",
    "# Process text: remove punctuation + stopwords\n",
    "cleaned_list = []\n",
    "\n",
    "for text in full_text_list:\n",
    "    if isinstance(text, str):  # make sure it's a string\n",
    "        # Step 1: remove punctuation\n",
    "        nopunc = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "        # Step 2: remove stopwords\n",
    "        clean_words = [word for word in nopunc.split() if word.lower() not in stop_words]\n",
    "\n",
    "        # Step 3: join cleaned words back\n",
    "        cleaned_list.append(' '.join(clean_words))\n",
    "    else:\n",
    "        cleaned_list.append('')  # if not string, just keep empty\n",
    "\n",
    "# Tokenize\n",
    "tokenized_article_list = [article.split() for article in cleaned_list]\n",
    "\n",
    "print(\"‚úÖ Text cleaned and tokenized.\")\n",
    "print(\"Example tokens from first article:\")\n",
    "print(tokenized_article_list[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "616ce9a0-db6d-4961-8d43-deaef9272ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Flash_Flood_Mentioned column created.\n",
      "                                           Full Text  Flash_Flood_Mentioned\n",
      "0  Parts of the United Kingdom continue to grappl...                      0\n",
      "1  Storms and heavy rain brought flash flooding t...                      1\n",
      "2  In the United Kingdom, intense downpours excee...                      1\n",
      "3  Thousands of trees are to be planted as part o...                      0\n",
      "4  England may be set to flood at the end of wint...                      0\n",
      "5  Police in UK report that one person is missing...                      1\n",
      "6  Hundreds of homes have been flooded in England...                      1\n",
      "7  Thunderstorms affected parts of western Europe...                      1\n",
      "8  Heavy rainfall in eastern England, UK on 09 Ju...                      1\n",
      "9  More than 300,000 homes in England are now bet...                      0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# STEP 4 ‚Äî DETECT 'FLASH FLOOD' MENTIONS\n",
    "# ====================================================\n",
    "\n",
    "\"\"\"\n",
    "We check if each article contains the word 'flash' followed by 'flood'\n",
    "to mark that article as mentioning a flash flood.\n",
    "\"\"\"\n",
    "\n",
    "df['Flash_Flood_Mentioned'] = [\n",
    "    1 if any(\n",
    "        article[i].lower() == 'flash'\n",
    "        and i + 1 < len(article)\n",
    "        and article[i + 1].lower().startswith('flood')\n",
    "        for i in range(len(article))\n",
    "    )\n",
    "    else 0\n",
    "    for article in tokenized_article_list\n",
    "]\n",
    "\n",
    "# Preview first few results\n",
    "print(\"‚úÖ Flash_Flood_Mentioned column created.\")\n",
    "print(df[['Full Text', 'Flash_Flood_Mentioned']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c895a1a-5360-43c9-86f7-6254c887f353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted rainfall, water level, storm name, and location.\n",
      "            Rainfall_mm                          WaterLevel_m  \\\n",
      "0                  None  1.79, 1.65, 2.22, 2.12, 30.52, 30.37   \n",
      "1                  41.5                2.85, 0.60, 1.55, 1.40   \n",
      "2            20, 20, 20                                  None   \n",
      "3                  None                                   400   \n",
      "4                  None                                  None   \n",
      "5             140, 71.4                                  None   \n",
      "6                  None     6.55, 7.04, 5.33, 5.56, 4.4, 3.97   \n",
      "7  26, 41.6, 34.6, 51.2                                  None   \n",
      "8                50, 90                                  None   \n",
      "9                  None                                  None   \n",
      "\n",
      "                                           StormName  \\\n",
      "0  Babet, Babet, Babet, Babet, Babet, Babet, Babe...   \n",
      "1                                               None   \n",
      "2                                               None   \n",
      "3                                               None   \n",
      "4                                               None   \n",
      "5                                               None   \n",
      "6                           Franklin, Dudley, Eunice   \n",
      "7                                               None   \n",
      "8                                               None   \n",
      "9                                               None   \n",
      "\n",
      "                                            Location  \n",
      "0  Aberdeenshire, Cobh, County Cork, Ordsal, Engl...  \n",
      "1  United Kingdom, Hertfordshire, Cambridgeshire,...  \n",
      "2           Scotland, UK, the United Kingdom, London  \n",
      "3  Great Britain‚Äôs, England, woodland, Castlehill...  \n",
      "4             England, Wessex, UK, US, Wales, London  \n",
      "5  Aberdeenshire, Dundee, England, Ladybank, Forf...  \n",
      "6  Yorkshire, Bewdley, South Yorkshire, the West ...  \n",
      "7  Schwyz, Wolhusen pic.twitter.com/81wiHOInL0, K...  \n",
      "8  Norfolk, Cambridgeshire, England, Peterborough...  \n",
      "9                   England, Suffolk, Boston, London  \n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# STEP 5 ‚Äî EXTRACT RAINFALL, WATER LEVEL, STORM, LOCATION\n",
    "# ====================================================\n",
    "\n",
    "# Regex patterns\n",
    "rainfall_pattern = re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(?:mm|millimetres|millimeters)', re.IGNORECASE)\n",
    "waterlevel_pattern = re.compile(r'(\\d+(?:\\.\\d+)?)\\s*(?:m|metres|meters)\\b', re.IGNORECASE)\n",
    "storm_pattern = re.compile(r'\\b(?:Storm|Cyclone|Hurricane|Typhoon)\\s+([A-Z][a-z]+)\\b')\n",
    "\n",
    "# Extraction function\n",
    "def extract_info(text):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    rainfall = rainfall_pattern.findall(text)\n",
    "    waterlevel = waterlevel_pattern.findall(text)\n",
    "    storm = storm_pattern.findall(text)\n",
    "\n",
    "    # NER for location\n",
    "    doc = nlp(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "\n",
    "    # Convert lists to readable strings\n",
    "    rainfall = ', '.join(rainfall) if rainfall else None\n",
    "    waterlevel = ', '.join(waterlevel) if waterlevel else None\n",
    "    storm = ', '.join(storm) if storm else None\n",
    "    locations = ', '.join(list(set(locations))) if locations else None\n",
    "\n",
    "    return rainfall, waterlevel, storm, locations\n",
    "\n",
    "# Apply extraction to every article\n",
    "df[['Rainfall_mm', 'WaterLevel_m', 'StormName', 'Location']] = df['Full Text'].apply(\n",
    "    lambda x: pd.Series(extract_info(x))\n",
    ")\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(\"‚úÖ Extracted rainfall, water level, storm name, and location.\")\n",
    "print(df[['Rainfall_mm', 'WaterLevel_m', 'StormName', 'Location']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "950e450d-f179-441d-adb0-2c401e801e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Flags created.\n",
      "   has_rainfall  has_waterlevel  has_any_numeric  Flash_Flood_Mentioned\n",
      "0         False            True             True                      0\n",
      "1          True            True             True                      1\n",
      "2          True           False             True                      1\n",
      "3         False            True             True                      0\n",
      "4         False           False            False                      0\n",
      "5          True           False             True                      1\n",
      "6         False            True             True                      1\n",
      "7          True           False             True                      1\n",
      "8          True           False             True                      1\n",
      "9         False           False            False                      0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# STEP 6 ‚Äî CREATE FLAGS FOR RAINFALL / WATER LEVEL / NUMERICS\n",
    "# ====================================================\n",
    "\n",
    "# Replace empty strings with NaN so .notna() works properly\n",
    "for col in ['Rainfall_mm', 'WaterLevel_m']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace('', np.nan)\n",
    "\n",
    "# Create flags\n",
    "df['has_rainfall'] = df['Rainfall_mm'].notna()\n",
    "df['has_waterlevel'] = df['WaterLevel_m'].notna()\n",
    "df['has_any_numeric'] = df['has_rainfall'] | df['has_waterlevel']\n",
    "\n",
    "# Make sure Flash_Flood_Mentioned is in 0/1 format\n",
    "df['Flash_Flood_Mentioned'] = (\n",
    "    pd.to_numeric(df['Flash_Flood_Mentioned'], errors='coerce')\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "print(\"‚úÖ Flags created.\")\n",
    "print(df[['has_rainfall', 'has_waterlevel', 'has_any_numeric', 'Flash_Flood_Mentioned']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "393db8f5-964b-486b-b56f-94f023b3e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Articles grouped successfully!\n",
      "\n",
      "Group\n",
      "OtherFlood (no numerics)    35\n",
      "OtherFlood + Numerics       27\n",
      "FlashFlood + Numerics       14\n",
      "FlashFlood (no numerics)     4\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Unique locations mentioned per group:\n",
      "\n",
      "‚Ä¢ OtherFlood + Numerics: 165 locations\n",
      "‚Ä¢ FlashFlood + Numerics: 131 locations\n",
      "‚Ä¢ OtherFlood (no numerics): 71 locations\n",
      "‚Ä¢ FlashFlood (no numerics): 23 locations\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# STEP 7 ‚Äî GROUP ARTICLES & GET QUICK INSIGHTS\n",
    "# ====================================================\n",
    "\n",
    "'''\n",
    "Group meanings:\n",
    "\n",
    "FlashFlood + Numerics       ‚Üí Articles that mention 'flash flood' and include numeric data (rainfall in mm or waterlevel in m).\n",
    "                              These are data-rich flash flood events and are most useful for detailed analysis.\n",
    "\n",
    "FlashFlood (no numerics)    ‚Üí Articles that mention 'flash flood' but have no numeric data.\n",
    "                              Qualitative flash flood reports; still useful for event occurrence mapping.\n",
    "\n",
    "OtherFlood + Numerics       ‚Üí Articles about floods (not flash floods) that include numeric data.\n",
    "                              Likely riverine or long-duration floods with measurable rainfall/levels.\n",
    "\n",
    "OtherFlood (no numerics)    ‚Üí General flood mentions with no numeric data.\n",
    "                              Least data-dense; useful mainly for regional flood frequency insights; more like 'human interest' stories.\n",
    "'''\n",
    "\n",
    "# --- Create the main grouping ---\n",
    "df['Group'] = np.select(\n",
    "    [\n",
    "        (df['Flash_Flood_Mentioned'] == 1) & (df['has_any_numeric'] == True),\n",
    "        (df['Flash_Flood_Mentioned'] == 1) & (df['has_any_numeric'] == False),\n",
    "        (df['Flash_Flood_Mentioned'] == 0) & (df['has_any_numeric'] == True)\n",
    "    ],\n",
    "    [\n",
    "        'FlashFlood + Numerics',\n",
    "        'FlashFlood (no numerics)',\n",
    "        'OtherFlood + Numerics'\n",
    "    ],\n",
    "    default='OtherFlood (no numerics)'\n",
    ")\n",
    "\n",
    "# --- Show overall counts ---\n",
    "print(\"‚úÖ Articles grouped successfully!\\n\")\n",
    "print(df['Group'].value_counts(), \"\\n\")\n",
    "\n",
    "# --- Get total unique locations per group ---\n",
    "group_locs = {}\n",
    "\n",
    "for grp in df['Group'].unique():\n",
    "    all_locs = []\n",
    "    for locs in df[df['Group'] == grp]['Location'].dropna():\n",
    "        parts = str(locs).split(',')\n",
    "        for p in parts:\n",
    "            p = p.strip()\n",
    "            if p != '':\n",
    "                all_locs.append(p)\n",
    "    group_locs[grp] = len(set(all_locs))\n",
    "\n",
    "print(\"Unique locations mentioned per group:\\n\")\n",
    "for g, n in group_locs.items():\n",
    "    print(f\"‚Ä¢ {g}: {n} locations\")\n",
    "\n",
    "# --- Top locations in 'FlashFlood + Numerics' group ---\n",
    "priority = df[df['Group'] == 'FlashFlood + Numerics']\n",
    "\n",
    "all_locs = []\n",
    "for locs in priority['Location'].dropna():\n",
    "    parts = str(locs).split(',')\n",
    "    for p in parts:\n",
    "        p = p.strip()\n",
    "        if p != '':\n",
    "            all_locs.append(p)\n",
    "\n",
    "location_counts = pd.Series(all_locs).value_counts()\n",
    "# print(\"\\nTop 10 locations in 'FlashFlood + Numerics' group:\\n\", location_counts, \"\\n\")\n",
    "\n",
    "# --- Show a few sample rows from that group ---\n",
    "cols = ['Full Text', 'Rainfall_mm', 'WaterLevel_m', 'StormName', 'Location']\n",
    "# print(\"Sample articles from 'FlashFlood + Numerics':\\n\")\n",
    "# print(priority[cols].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ce913dd-8e8a-4384-ac09-fdd2bcb29f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(location_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a70b370-c810-4448-b393-478b78443b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full dataset saved as: flood_articles_processed.csv\n",
      "‚úÖ Priority dataset (FlashFlood + Numerics) saved as: flood_articles_flashflood_numeric.csv\n",
      "‚úÖ Summary text file saved as: flood_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# STEP 8 ‚Äî SAVE OUTPUTS\n",
    "# ====================================================\n",
    "\n",
    "# File names (edit if you want)\n",
    "output_csv = 'flood_articles_processed.csv'\n",
    "priority_csv = 'flood_articles_flashflood_numeric.csv'\n",
    "\n",
    "# Save the full dataframe\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(\"‚úÖ Full dataset saved as:\", output_csv)\n",
    "\n",
    "# Save only the priority group (FlashFlood + Numerics)\n",
    "priority = df[df['Group'] == 'FlashFlood + Numerics']\n",
    "priority.to_csv(priority_csv, index=False)\n",
    "print(\"‚úÖ Priority dataset (FlashFlood + Numerics) saved as:\", priority_csv)\n",
    "\n",
    "# Optional quick summary file\n",
    "summary_txt = 'flood_summary.txt'\n",
    "with open(summary_txt, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Flood Information Extraction Summary\\n\")\n",
    "    f.write(\"===================================\\n\\n\")\n",
    "    f.write(str(df['Group'].value_counts()))\n",
    "    f.write(\"\\n\\nTop locations (FlashFlood + Numerics):\\n\")\n",
    "    f.write(str(location_counts.head(40)))\n",
    "print(\"‚úÖ Summary text file saved as:\", summary_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6369a58d-efd2-4d0b-ad7f-7470217c7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_txt = 'flood_summary.txt'\n",
    "\n",
    "# print(\"üìÑ Contents of flood_summary.txt:\\n\")\n",
    "# with open(summary_txt, 'r', encoding='utf-8') as f:\n",
    "#     print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8f795-ded5-4351-a371-463a9eacf544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
